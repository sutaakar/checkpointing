apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  labels:
    kueue.x-k8s.io/queue-name: lq
  name: sft
spec:
  nprocPerNode: '1'
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: 'false'
        spec:
          terminationGracePeriodSeconds: 120
          containers:
            - args:
                - |-

                  program_path=$(mktemp -d)
                  read -r -d '' SCRIPT << EOM

                  def main(parameters):
                      import random
                      import os
                      import torch
                      import numpy
                      from numpy.core.multiarray import _reconstruct
                      import torch.serialization
                      torch.serialization.add_safe_globals([_reconstruct, numpy.ndarray, numpy.dtype, numpy.dtypes.UInt32DType])
                      from datetime import datetime
                      import signal
                      import torch.distributed as dist

                      from datasets import load_dataset
                      from transformers import (
                          AutoTokenizer,
                          TrainingArguments,
                          TrainerState,
                          TrainerControl,
                          TrainerCallback,
                          set_seed,
                      )
                      from transformers.trainer_utils import get_last_checkpoint

                      from trl import (
                          ModelConfig,
                          ScriptArguments,
                          SFTConfig,
                          SFTTrainer,
                          TrlParser,
                          get_peft_config,
                          get_quantization_config,
                          get_kbit_device_map,
                      )

                      class SigtermCheckpointCallback(TrainerCallback):
                          """
                          A custom callback to save a checkpoint when SIGTERM is received on any pod,
                          with distributed coordination across all ranks.
                          """
                          def __init__(self, output_dir: str):
                              self.output_dir = output_dir
                              self.checkpoint_requested = False
                              self.save_triggered = False
                              self.checkpoint_stream = None
                              self.sigterm_tensor = None  # For distributed signaling

                          def _log_message(self, message: str):
                              """Helper to print messages with a timestamp."""
                              timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                              print(f"[{timestamp}] {message}")

                          def _init_distributed_signal_tensor(self):
                              """Initialize tensor for distributed SIGTERM signaling."""
                              try:
                                  if dist.is_initialized():
                                      device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')
                                      self.sigterm_tensor = torch.zeros(1, dtype=torch.float32, device=device)
                                      self._log_message(f"Initialized distributed SIGTERM tensor on device: {device}")
                                  else:
                                      self._log_message("Distributed training not initialized - using local SIGTERM handling only")
                              except Exception as e:
                                  self._log_message(f"Failed to initialize distributed SIGTERM tensor: {e}. Using local handling only.")

                          def _check_distributed_sigterm(self):
                              """Check if any rank has received SIGTERM."""
                              try:
                                  if dist.is_initialized() and self.sigterm_tensor is not None:
                                      # Check if any rank has signaled SIGTERM
                                      dist.all_reduce(self.sigterm_tensor, op=dist.ReduceOp.MAX)
                                      return self.sigterm_tensor.item() > 0.5
                              except Exception as e:
                                  # If distributed communication fails, only rely on local signals
                                  self._log_message(f"Distributed SIGTERM check failed: {e}. Using local signal only.")
                              return True

                          def _sigterm_handler(self, signum, frame):
                              """Sets a flag and updates the tensor to indicate that a SIGTERM signal was received."""
                              rank = os.environ.get("RANK", "-1")
                              self._log_message(f"Rank {rank}: SIGTERM received, flagging for checkpoint.")
                              self.checkpoint_requested = True
                              if self.sigterm_tensor is not None:
                                  self.sigterm_tensor.fill_(1.0)

                          def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              rank = os.environ.get("RANK", "-1")

                              # All ranks should ensure output directory exists
                              os.makedirs(self.output_dir, exist_ok=True)

                              # Initialize distributed signaling tensor for all ranks
                              self._init_distributed_signal_tensor()

                              # All ranks register SIGTERM handler for distributed coordination
                              if torch.cuda.is_available():
                                  self.checkpoint_stream = torch.cuda.Stream()
                                  self._log_message(f"Rank {rank}: Created dedicated CUDA stream for checkpointing.")

                              # Set up SIGTERM signal handler on all ranks
                              signal.signal(signal.SIGTERM, self._sigterm_handler)
                              self._log_message(f"Rank {rank}: SIGTERM signal handler registered for distributed coordination.")

                              # Synchronize all ranks to ensure distributed setup is complete
                              try:
                                  if dist.is_initialized():
                                      dist.barrier()
                                      self._log_message(f"Rank {rank}: Distributed coordination setup synchronized across all ranks")
                              except Exception as e:
                                  self._log_message(f"Rank {rank}: Failed to synchronize distributed setup: {e}")


                          def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              # Check if any rank has received the SIGTERM signal
                              if self._check_distributed_sigterm() and not self.save_triggered:
                                  rank = os.environ.get("RANK", "-1")
                                  self._log_message(f"Rank {rank}: Distributed SIGTERM detected, initiating checkpoint at step {state.global_step}.")
                                  self.save_triggered = True  # Prevent multiple saves
                                  control.should_save = True
                                  control.should_training_stop = True

                          def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              rank = os.environ.get("RANK", "-1")
                              if rank != "0":
                                  return
                              if self.checkpoint_requested:
                                  self._log_message(f"Rank {rank}: Training ended due to distributed SIGTERM checkpoint request. Final checkpoint should have been saved.")

                          def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              rank = os.environ.get("RANK", "-1")
                              if rank != "0":
                                  return
                              self._log_message(f"Rank {rank}: Checkpoint save completed.")
                              if self.checkpoint_requested:
                                  self._log_message(f"Rank {rank}: Distributed SIGTERM-triggered checkpoint save finished successfully.")


                      parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))
                      script_args, training_args, model_args = parser.parse_dict(parameters)

                      # Set seed for reproducibility
                      set_seed(training_args.seed)

                      # Model and tokenizer
                      quantization_config = get_quantization_config(model_args)
                      model_kwargs = dict(
                          revision=model_args.model_revision,
                          trust_remote_code=model_args.trust_remote_code,
                          attn_implementation=model_args.attn_implementation,
                          torch_dtype=model_args.torch_dtype,
                          use_cache=False if training_args.gradient_checkpointing or
                                             training_args.fsdp_config.get("activation_checkpointing",
                                                                           False) else True,
                          device_map=get_kbit_device_map() if quantization_config is not None else None,
                          quantization_config=quantization_config,
                      )
                      training_args.model_init_kwargs = model_kwargs
                      tokenizer = AutoTokenizer.from_pretrained(
                          model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, use_fast=True
                      )
                      if tokenizer.pad_token is None:
                          # Models like Llama 3 use a dedicated padding token
                          right_pad_id = tokenizer.convert_tokens_to_ids('<|finetune_right_pad_id|>')
                          if right_pad_id is not None:
                              tokenizer.pad_token = '<|finetune_right_pad_id|>'
                          else:
                              tokenizer.pad_token = tokenizer.eos_token

                      # Chat template
                      # You may need to provide your own chat template if the model does not have a default one
                      # or if you want to customize it
                      # Llama 3 instruct template, make sure to add "lm_head" and "embed_tokens" layers to lora_modules_to_save
                      # LLAMA_3_CHAT_TEMPLATE="{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
                      # Anthropic/Vicuna like template without the need for special tokens
                      # LLAMA_3_CHAT_TEMPLATE = (
                      #     "{% for message in messages %}"
                      #     "{% if message['role'] == 'system' %}"
                      #     "{{ message['content'] }}"
                      #     "{% elif message['role'] == 'user' %}"
                      #     "{{ '\n\nHuman: ' + message['content'] +  eos_token }}"
                      #     "{% elif message['role'] == 'assistant' %}"
                      #     "{{ '\n\nAssistant: '  + message['content'] +  eos_token  }}"
                      #     "{% endif %}"
                      #     "{% endfor %}"
                      #     "{% if add_generation_prompt %}"
                      #     "{{ '\n\nAssistant: ' }}"
                      #     "{% endif %}"
                      # )
                      # tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE

                      # Datasets
                      train_dataset = load_dataset(
                          path=script_args.dataset_name,
                          name=script_args.dataset_config,
                          split=script_args.dataset_train_split,
                      )
                      test_dataset = None
                      if training_args.eval_strategy != "no":
                          test_dataset = load_dataset(
                              path=script_args.dataset_name,
                              name=script_args.dataset_config,
                              split=script_args.dataset_test_split,
                          )

                      # Templatize datasets
                      # You may need to adjust the mapping between columns and the chat template
                      def template_dataset(sample):
                          # return {"text": tokenizer.apply_chat_template(examples["messages"], tokenize=False)}
                          messages = [
                              {"role": "user", "content": sample['question']},
                              {"role": "assistant", "content": sample['answer']},
                          ]
                          return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

                      train_dataset = train_dataset.map(template_dataset, remove_columns=["question", "answer"])
                      if training_args.eval_strategy != "no":
                          # test_dataset = test_dataset.map(template_dataset, remove_columns=["messages"])
                          test_dataset = test_dataset.map(template_dataset, remove_columns=["question", "answer"])

                      # Check random samples
                      with training_args.main_process_first(
                          desc="Log few samples from the training set"
                      ):
                          for index in random.sample(range(len(train_dataset)), 2):
                              print(train_dataset[index]["text"])

                      # Training
                      trainer = SFTTrainer(
                          model=model_args.model_name_or_path,
                          args=training_args,
                          train_dataset=train_dataset,
                          eval_dataset=test_dataset,
                          peft_config=get_peft_config(model_args),
                          processing_class=tokenizer,
                          callbacks=[SigtermCheckpointCallback(training_args.output_dir)],
                      )

                      if trainer.accelerator.is_main_process and hasattr(trainer.model, "print_trainable_parameters"):
                          trainer.model.print_trainable_parameters()

                      checkpoint = get_last_checkpoint(training_args.output_dir)
                      if checkpoint is None:
                          print("No checkpoint found, starting training from scratch.")

                      trainer.train(resume_from_checkpoint=checkpoint)

                      trainer.save_model(training_args.output_dir)

                      with training_args.main_process_first(desc="Training completed"):
                          print(f"Training completed, model checkpoint written to {training_args.output_dir}")

                  main({'model_name_or_path': 'meta-llama/Llama-3.2-1B-Instruct', 'model_revision': 'main', 'torch_dtype': 'bfloat16', 'attn_implementation': 'flash_attention_2', 'use_liger': False, 'use_peft': True, 'lora_r': 16, 'lora_alpha': 8, 'lora_dropout': 0.05, 'lora_target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], 'lora_modules_to_save': [], 'load_in_4bit': False, 'load_in_8bit': False, 'dataset_name': 'gsm8k', 'dataset_config': 'main', 'dataset_train_split': 'train', 'dataset_test_split': 'test', 'dataset_text_field': 'text', 'dataset_kwargs': {'add_special_tokens': False, 'append_concat_token': False}, 'max_seq_length': 1024, 'dataset_batch_size': 1000, 'packing': False, 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'auto_find_batch_size': False, 'eval_strategy': 'epoch', 'bf16': True, 'tf32': False, 'learning_rate': 0.0002, 'warmup_steps': 10, 'lr_scheduler_type': 'inverse_sqrt', 'optim': 'adamw_torch_fused', 'max_grad_norm': 1.0, 'seed': 42, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': {'use_reentrant': False}, 'fsdp': 'full_shard auto_wrap', 'fsdp_config': {'activation_checkpointing': True, 'cpu_ram_efficient_loading': False, 'sync_module_states': True, 'use_orig_params': True, 'limit_all_gathers': False}, 'save_strategy': 'no', 'save_total_limit': 1, 'log_level': 'warning', 'logging_strategy': 'steps', 'logging_steps': 1, 'report_to': ['tensorboard'], 'output_dir': '/mnt/shared/Meta-Llama-3.1-8B-Instruct'})

                  EOM
                  printf "%s" "$SCRIPT" > "$program_path/ephemeral_script.py"
                  torchrun "$program_path/ephemeral_script.py"
              command:
                - bash
                - '-c'
              env:
                - name: HF_HOME
                  value: /mnt/shared/.cache
                - name: HF_TOKEN
                  valueFrom:
                    secretKeyRef:
                      key: HF_TOKEN
                      name: hf-token
                - name: NCCL_DEBUG
                  value: INFO
              image: 'quay.io/modh/training:py311-cuda124-torch251'
              name: pytorch
              resources:
                limits:
                  cpu: '4'
                  memory: 64Gi
                  nvidia.com/gpu: '1'
                requests:
                  cpu: '4'
                  memory: 64Gi
                  nvidia.com/gpu: '1'
              volumeMounts:
                - mountPath: /mnt/shared
                  name: shared
          volumes:
            - name: shared
              persistentVolumeClaim:
                claimName: shared
    Worker:
      replicas: 1
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: 'false'
        spec:
          terminationGracePeriodSeconds: 120
          containers:
            - args:
                - |-

                  program_path=$(mktemp -d)
                  read -r -d '' SCRIPT << EOM

                  def main(parameters):
                      import random
                      import os
                      import torch
                      import numpy
                      from numpy.core.multiarray import _reconstruct
                      import torch.serialization
                      torch.serialization.add_safe_globals([_reconstruct, numpy.ndarray, numpy.dtype, numpy.dtypes.UInt32DType])
                      from datetime import datetime
                      import signal
                      import torch.distributed as dist

                      from datasets import load_dataset
                      from transformers import (
                          AutoTokenizer,
                          TrainingArguments,
                          TrainerState,
                          TrainerControl,
                          TrainerCallback,
                          set_seed,
                      )
                      from transformers.trainer_utils import get_last_checkpoint

                      from trl import (
                          ModelConfig,
                          ScriptArguments,
                          SFTConfig,
                          SFTTrainer,
                          TrlParser,
                          get_peft_config,
                          get_quantization_config,
                          get_kbit_device_map,
                      )

                      class SigtermCheckpointCallback(TrainerCallback):
                          """
                          A custom callback to save a checkpoint when SIGTERM is received on any pod,
                          with distributed coordination across all ranks.
                          """
                          def __init__(self, output_dir: str):
                              self.output_dir = output_dir
                              self.checkpoint_requested = False
                              self.save_triggered = False
                              self.checkpoint_stream = None
                              self.sigterm_tensor = None  # For distributed signaling

                          def _log_message(self, message: str):
                              """Helper to print messages with a timestamp."""
                              timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                              print(f"[{timestamp}] {message}")

                          def _init_distributed_signal_tensor(self):
                              """Initialize tensor for distributed SIGTERM signaling."""
                              try:
                                  if dist.is_initialized():
                                      device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')
                                      self.sigterm_tensor = torch.zeros(1, dtype=torch.float32, device=device)
                                      self._log_message(f"Initialized distributed SIGTERM tensor on device: {device}")
                                  else:
                                      self._log_message("Distributed training not initialized - using local SIGTERM handling only")
                              except Exception as e:
                                  self._log_message(f"Failed to initialize distributed SIGTERM tensor: {e}. Using local handling only.")

                          def _check_distributed_sigterm(self):
                              """Check if any rank has received SIGTERM."""
                              try:
                                  if dist.is_initialized() and self.sigterm_tensor is not None:
                                      # Check if any rank has signaled SIGTERM
                                      dist.all_reduce(self.sigterm_tensor, op=dist.ReduceOp.MAX)
                                      return self.sigterm_tensor.item() > 0.5
                              except Exception as e:
                                  # If distributed communication fails, only rely on local signals
                                  self._log_message(f"Distributed SIGTERM check failed: {e}. Using local signal only.")
                              return True

                          def _sigterm_handler(self, signum, frame):
                              """Sets a flag and updates the tensor to indicate that a SIGTERM signal was received."""
                              rank = os.environ.get("RANK", "-1")
                              self._log_message(f"Rank {rank}: SIGTERM received, flagging for checkpoint.")
                              self.checkpoint_requested = True
                              if self.sigterm_tensor is not None:
                                  self.sigterm_tensor.fill_(1.0)

                          def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              rank = os.environ.get("RANK", "-1")

                              # All ranks should ensure output directory exists
                              os.makedirs(self.output_dir, exist_ok=True)

                              # Initialize distributed signaling tensor for all ranks
                              self._init_distributed_signal_tensor()

                              # All ranks register SIGTERM handler for distributed coordination
                              if torch.cuda.is_available():
                                  self.checkpoint_stream = torch.cuda.Stream()
                                  self._log_message(f"Rank {rank}: Created dedicated CUDA stream for checkpointing.")

                              # Set up SIGTERM signal handler on all ranks
                              signal.signal(signal.SIGTERM, self._sigterm_handler)
                              self._log_message(f"Rank {rank}: SIGTERM signal handler registered for distributed coordination.")

                              # Synchronize all ranks to ensure distributed setup is complete
                              try:
                                  if dist.is_initialized():
                                      dist.barrier()
                                      self._log_message(f"Rank {rank}: Distributed coordination setup synchronized across all ranks")
                              except Exception as e:
                                  self._log_message(f"Rank {rank}: Failed to synchronize distributed setup: {e}")


                          def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              # Check if any rank has received the SIGTERM signal
                              if self._check_distributed_sigterm() and not self.save_triggered:
                                  rank = os.environ.get("RANK", "-1")
                                  self._log_message(f"Rank {rank}: Distributed SIGTERM detected, initiating checkpoint at step {state.global_step}.")
                                  self.save_triggered = True  # Prevent multiple saves
                                  control.should_save = True
                                  control.should_training_stop = True

                          def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              rank = os.environ.get("RANK", "-1")
                              if rank != "0":
                                  return
                              if self.checkpoint_requested:
                                  self._log_message(f"Rank {rank}: Training ended due to distributed SIGTERM checkpoint request. Final checkpoint should have been saved.")

                          def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
                              rank = os.environ.get("RANK", "-1")
                              if rank != "0":
                                  return
                              self._log_message(f"Rank {rank}: Checkpoint save completed.")
                              if self.checkpoint_requested:
                                  self._log_message(f"Rank {rank}: Distributed SIGTERM-triggered checkpoint save finished successfully.")


                      parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))
                      script_args, training_args, model_args = parser.parse_dict(parameters)

                      # Set seed for reproducibility
                      set_seed(training_args.seed)

                      # Model and tokenizer
                      quantization_config = get_quantization_config(model_args)
                      model_kwargs = dict(
                          revision=model_args.model_revision,
                          trust_remote_code=model_args.trust_remote_code,
                          attn_implementation=model_args.attn_implementation,
                          torch_dtype=model_args.torch_dtype,
                          use_cache=False if training_args.gradient_checkpointing or
                                             training_args.fsdp_config.get("activation_checkpointing",
                                                                           False) else True,
                          device_map=get_kbit_device_map() if quantization_config is not None else None,
                          quantization_config=quantization_config,
                      )
                      training_args.model_init_kwargs = model_kwargs
                      tokenizer = AutoTokenizer.from_pretrained(
                          model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, use_fast=True
                      )
                      if tokenizer.pad_token is None:
                          # Models like Llama 3 use a dedicated padding token
                          right_pad_id = tokenizer.convert_tokens_to_ids('<|finetune_right_pad_id|>')
                          if right_pad_id is not None:
                              tokenizer.pad_token = '<|finetune_right_pad_id|>'
                          else:
                              tokenizer.pad_token = tokenizer.eos_token

                      # Chat template
                      # You may need to provide your own chat template if the model does not have a default one
                      # or if you want to customize it
                      # Llama 3 instruct template, make sure to add "lm_head" and "embed_tokens" layers to lora_modules_to_save
                      # LLAMA_3_CHAT_TEMPLATE="{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
                      # Anthropic/Vicuna like template without the need for special tokens
                      # LLAMA_3_CHAT_TEMPLATE = (
                      #     "{% for message in messages %}"
                      #     "{% if message['role'] == 'system' %}"
                      #     "{{ message['content'] }}"
                      #     "{% elif message['role'] == 'user' %}"
                      #     "{{ '\n\nHuman: ' + message['content'] +  eos_token }}"
                      #     "{% elif message['role'] == 'assistant' %}"
                      #     "{{ '\n\nAssistant: '  + message['content'] +  eos_token  }}"
                      #     "{% endif %}"
                      #     "{% endfor %}"
                      #     "{% if add_generation_prompt %}"
                      #     "{{ '\n\nAssistant: ' }}"
                      #     "{% endif %}"
                      # )
                      # tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE

                      # Datasets
                      train_dataset = load_dataset(
                          path=script_args.dataset_name,
                          name=script_args.dataset_config,
                          split=script_args.dataset_train_split,
                      )
                      test_dataset = None
                      if training_args.eval_strategy != "no":
                          test_dataset = load_dataset(
                              path=script_args.dataset_name,
                              name=script_args.dataset_config,
                              split=script_args.dataset_test_split,
                          )

                      # Templatize datasets
                      # You may need to adjust the mapping between columns and the chat template
                      def template_dataset(sample):
                          # return {"text": tokenizer.apply_chat_template(examples["messages"], tokenize=False)}
                          messages = [
                              {"role": "user", "content": sample['question']},
                              {"role": "assistant", "content": sample['answer']},
                          ]
                          return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

                      train_dataset = train_dataset.map(template_dataset, remove_columns=["question", "answer"])
                      if training_args.eval_strategy != "no":
                          # test_dataset = test_dataset.map(template_dataset, remove_columns=["messages"])
                          test_dataset = test_dataset.map(template_dataset, remove_columns=["question", "answer"])

                      # Check random samples
                      with training_args.main_process_first(
                          desc="Log few samples from the training set"
                      ):
                          for index in random.sample(range(len(train_dataset)), 2):
                              print(train_dataset[index]["text"])

                      # Training
                      trainer = SFTTrainer(
                          model=model_args.model_name_or_path,
                          args=training_args,
                          train_dataset=train_dataset,
                          eval_dataset=test_dataset,
                          peft_config=get_peft_config(model_args),
                          processing_class=tokenizer,
                          callbacks=[SigtermCheckpointCallback(training_args.output_dir)],
                      )

                      if trainer.accelerator.is_main_process and hasattr(trainer.model, "print_trainable_parameters"):
                          trainer.model.print_trainable_parameters()

                      checkpoint = get_last_checkpoint(training_args.output_dir)
                      if checkpoint is None:
                          print("No checkpoint found, starting training from scratch.")

                      trainer.train(resume_from_checkpoint=checkpoint)

                      trainer.save_model(training_args.output_dir)

                      with training_args.main_process_first(desc="Training completed"):
                          print(f"Training completed, model checkpoint written to {training_args.output_dir}")

                  main({'model_name_or_path': 'meta-llama/Llama-3.2-1B-Instruct', 'model_revision': 'main', 'torch_dtype': 'bfloat16', 'attn_implementation': 'flash_attention_2', 'use_liger': False, 'use_peft': True, 'lora_r': 16, 'lora_alpha': 8, 'lora_dropout': 0.05, 'lora_target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], 'lora_modules_to_save': [], 'load_in_4bit': False, 'load_in_8bit': False, 'dataset_name': 'gsm8k', 'dataset_config': 'main', 'dataset_train_split': 'train', 'dataset_test_split': 'test', 'dataset_text_field': 'text', 'dataset_kwargs': {'add_special_tokens': False, 'append_concat_token': False}, 'max_seq_length': 1024, 'dataset_batch_size': 1000, 'packing': False, 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'auto_find_batch_size': False, 'eval_strategy': 'epoch', 'bf16': True, 'tf32': False, 'learning_rate': 0.0002, 'warmup_steps': 10, 'lr_scheduler_type': 'inverse_sqrt', 'optim': 'adamw_torch_fused', 'max_grad_norm': 1.0, 'seed': 42, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': {'use_reentrant': False}, 'fsdp': 'full_shard auto_wrap', 'fsdp_config': {'activation_checkpointing': True, 'cpu_ram_efficient_loading': False, 'sync_module_states': True, 'use_orig_params': True, 'limit_all_gathers': False}, 'save_strategy': 'no', 'save_total_limit': 1, 'log_level': 'warning', 'logging_strategy': 'steps', 'logging_steps': 1, 'report_to': ['tensorboard'], 'output_dir': '/mnt/shared/Meta-Llama-3.1-8B-Instruct'})

                  EOM
                  printf "%s" "$SCRIPT" > "$program_path/ephemeral_script.py"
                  torchrun "$program_path/ephemeral_script.py"
              command:
                - bash
                - '-c'
              env:
                - name: HF_HOME
                  value: /mnt/shared/.cache
                - name: HF_TOKEN
                  valueFrom:
                    secretKeyRef:
                      key: HF_TOKEN
                      name: hf-token
                - name: NCCL_DEBUG
                  value: INFO
              image: 'quay.io/modh/training:py311-cuda124-torch251'
              name: pytorch
              resources:
                limits:
                  cpu: '4'
                  memory: 64Gi
                  nvidia.com/gpu: '1'
                requests:
                  cpu: '4'
                  memory: 64Gi
                  nvidia.com/gpu: '1'
              volumeMounts:
                - mountPath: /mnt/shared
                  name: shared
          volumes:
            - name: shared
              persistentVolumeClaim:
                claimName: shared
  runPolicy:
    suspend: false
